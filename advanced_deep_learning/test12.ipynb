{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPvgsqzykGyjoYHb+DHA+ZI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Faster R-CNN\n","- 데이터셋 다운로드만 한 상태로 loader 부분 코드 수정해서 실습 진행하면 되지만, 오래 걸려서 따로 해볼 것"],"metadata":{"id":"iB0aQ9y4kO09"}},{"cell_type":"code","source":["# 필요한 파일들 다운로드\n","!wget http://images.cocodataset.org/zips/train2017.zip\n","!wget http://images.cocodataset.org/zips/val2017.zip\n","!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3-Apj5hrWUE","executionInfo":{"status":"ok","timestamp":1747878910399,"user_tz":-540,"elapsed":402919,"user":{"displayName":"yam kk","userId":"06700488700849752406"}},"outputId":"a114e88a-1a0a-42e4-8667-c9cae0b0e1c6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-05-22 10:48:28--  http://images.cocodataset.org/zips/train2017.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 16.15.185.235, 3.5.28.57, 52.216.60.193, ...\n","Connecting to images.cocodataset.org (images.cocodataset.org)|16.15.185.235|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19336861798 (18G) [application/zip]\n","Saving to: ‘train2017.zip’\n","\n","train2017.zip       100%[===================>]  18.01G  35.0MB/s    in 6m 23s  \n","\n","2025-05-22 10:54:51 (48.2 MB/s) - ‘train2017.zip’ saved [19336861798/19336861798]\n","\n","--2025-05-22 10:54:51--  http://images.cocodataset.org/zips/val2017.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 16.15.200.222, 52.217.124.41, 16.15.177.183, ...\n","Connecting to images.cocodataset.org (images.cocodataset.org)|16.15.200.222|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 815585330 (778M) [application/zip]\n","Saving to: ‘val2017.zip’\n","\n","val2017.zip         100%[===================>] 777.80M  53.2MB/s    in 15s     \n","\n","2025-05-22 10:55:06 (52.9 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n","\n","--2025-05-22 10:55:06--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.235.57, 52.216.237.51, 52.217.120.169, ...\n","Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.235.57|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 252907541 (241M) [application/zip]\n","Saving to: ‘annotations_trainval2017.zip’\n","\n","annotations_trainva 100%[===================>] 241.19M  56.1MB/s    in 4.8s    \n","\n","2025-05-22 10:55:11 (50.6 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n","\n"]}]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","# 경로 설정\n","dataset_dir = '/content/drive/MyDrive/cbnu/datasets/12/coco'\n","train_dir = os.path.join(dataset_dir, 'train2017')\n","val_dir = os.path.join(dataset_dir, 'val2017')\n","annotations_dir = os.path.join(dataset_dir, 'annotations')\n","\n","# 디렉터리 생성\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(val_dir, exist_ok=True)\n","os.makedirs(annotations_dir, exist_ok=True)\n","\n","# 압축 해제\n","with zipfile.ZipFile('train2017.zip', 'r') as zip_ref:\n","    zip_ref.extractall(train_dir)\n","\n","with zipfile.ZipFile('val2017.zip', 'r') as zip_ref:\n","    zip_ref.extractall(val_dir)\n","\n","with zipfile.ZipFile('annotations_trainval2017.zip', 'r') as zip_ref:\n","    zip_ref.extractall(annotations_dir)\n"],"metadata":{"id":"Fe_eYoYRtB3y","executionInfo":{"status":"ok","timestamp":1747912478673,"user_tz":-540,"elapsed":33499527,"user":{"displayName":"yam kk","userId":"06700488700849752406"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from PIL import Image\n","from pycocotools.coco import COCO\n","from torch.utils.data import Dataset\n","\n","\n","class COCODataset(Dataset):\n","    def __init__(self, root, train, transform=None):\n","        super().__init__()\n","        directory = \"train2017\" if train else \"val2017\"\n","        annotations = os.path.join(root, \"annotations\", f\"instances_{directory}.json\")\n","\n","        self.coco = COCO(annotations)\n","        self.image_path = os.path.join(root, directory)\n","        self.transform = transform\n","\n","        self.categories = self._get_categories()\n","        self.data = self._load_data()\n","\n","    def _get_categories(self):\n","        categories = {0: \"background\"}\n","        for category in self.coco.cats.values():\n","            categories[category[\"id\"]] = category[\"name\"]\n","        return categories\n","\n","    def _load_data(self):\n","        data = []\n","        for _id in self.coco.imgs:\n","            file_name = self.coco.loadImgs(_id)[0][\"file_name\"]\n","            image_path = os.path.join(self.image_path, file_name)\n","            image = Image.open(image_path).convert(\"RGB\")\n","\n","            boxes = []\n","            labels = []\n","            anns = self.coco.loadAnns(self.coco.getAnnIds(_id))\n","            for ann in anns:\n","                x, y, w, h = ann[\"bbox\"]\n","\n","                boxes.append([x, y, x + w, y + h])\n","                labels.append(ann[\"category_id\"])\n","\n","            target = {\n","            \"image_id\": torch.LongTensor([_id]),\n","                \"boxes\": torch.FloatTensor(boxes),\n","                \"labels\": torch.LongTensor(labels)\n","            }\n","            data.append([image, target])\n","        return data\n","\n","    def __getitem__(self, index):\n","        image, target = self.data[index]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"vqGG9EkGkPAF","executionInfo":{"status":"ok","timestamp":1747912484499,"user_tz":-540,"elapsed":49,"user":{"displayName":"yam kk","userId":"06700488700849752406"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","\n","def collator(batch):\n","    return tuple(zip(*batch))\n","\n","transform = transforms.Compose(\n","    [\n","        transforms.PILToTensor(),\n","        transforms.ConvertImageDtype(dtype=torch.float)\n","    ]\n",")\n","\n","# COCO 데이터셋 경로 수정\n","dataset_root = \"/content/drive/MyDrive/cbnu/datasets/12/coco\"\n","\n","# train과 test 데이터셋 경로를 dataset_root로 수정하여 인스턴스화\n","train_dataset = COCODataset(dataset_root, train=True, transform=transform)\n","test_dataset = COCODataset(dataset_root, train=False, transform=transform)\n","\n","# DataLoader 설정\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collator)\n","test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"id":"IMvAyWMWowYN","executionInfo":{"status":"error","timestamp":1747912773307,"user_tz":-540,"elapsed":36917,"user":{"displayName":"yam kk","userId":"06700488700849752406"}},"outputId":"b5732951-cf2e-4b67-874c-207b5e368016"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=34.71s)\n","creating index...\n","index created!\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/cbnu/datasets/12/coco/train2017/000000391895.jpg'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-8aeb863d4118>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# train과 test 데이터셋 경로를 dataset_root로 수정하여 인스턴스화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCODataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCODataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-a15281147595>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-a15281147595>\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadImgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/cbnu/datasets/12/coco/train2017/000000391895.jpg'"]}]},{"cell_type":"code","source":["from torchvision import models\n","from torchvision import ops\n","from torchvision.models.detection import rpn\n","from torchvision.models.detection import FasterRCNN\n","\n","\n","\n","backbone = models.vgg(weights=\"VGG16_Weights.IMAGENET1K_V1\").features\n","backbone.out_channels = 512\n","\n","anchor_generator = rpn.AnchorGenerator(\n","    sizes=((32, 64, 128, 256, 512),),\n","    aspect_ratios=((0.5, 1.0, 2.0),)\n",")\n","roi_pooler = ops.MultiScaleRoIAlign(\n","    featmap_names=[\"0\"],\n","    output_size=(7, 7),\n","    sampling_ratio=2\n",")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = FasterRCNN(\n","    backbone=backbone,\n","    num_classes=3,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",").to(device)"],"metadata":{"id":"Ss4vgB2Eox4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import optim\n","\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n","lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"],"metadata":{"id":"V6p2grDmoy-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1):\n","    cost = 0.0\n","    for idx, (images, targets) in enumerate(train_dataloader):\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        cost += losses\n","\n","    lr_scheduler.step()\n","    cost = cost / len(train_dataloader)\n","    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"],"metadata":{"id":"CIv9hg_5oz5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","from torchvision.transforms.functional import to_pil_image\n","\n","\n","def draw_bbox(ax, box, text, color):\n","    ax.add_patch(\n","        plt.Rectangle(\n","            xy=(box[0], box[1]),\n","            width=box[2] - box[0],\n","            height=box[3] - box[1],\n","            fill=False,\n","            edgecolor=color,\n","            linewidth=2,\n","        )\n","    )\n","    ax.annotate(\n","        text=text,\n","        xy=(box[0] - 5, box[1] - 5),\n","        color=color,\n","        weight=\"bold\",\n","        fontsize=13,\n","    )\n","\n","threshold = 0.5\n","categories = test_dataset.categories\n","with torch.no_grad():\n","    model.eval()\n","    for images, targets in test_dataloader:\n","        images = [image.to(device) for image in images]\n","        outputs = model(images)\n","\n","        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n","        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n","        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n","\n","        boxes = boxes[scores >= threshold].astype(np.int32)\n","        labels = labels[scores >= threshold]\n","        scores = scores[scores >= threshold]\n","\n","        fig = plt.figure(figsize=(8, 8))\n","        ax = fig.add_subplot(1, 1, 1)\n","        plt.imshow(to_pil_image(images[0]))\n","\n","        for box, label, score in zip(boxes, labels, scores):\n","            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")\n","\n","        tboxes = targets[0][\"boxes\"].numpy()\n","        tlabels = targets[0][\"labels\"].numpy()\n","        for box, label in zip(tboxes, tlabels):\n","            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")\n","\n","        plt.show()"],"metadata":{"id":"Ev9CjVHwo2CT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## YOLOv8\n","- ultralytics 다운로드 필요"],"metadata":{"id":"b7dkHiPPkW9O"}},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TGqghtV0vC98","executionInfo":{"status":"ok","timestamp":1747913224308,"user_tz":-540,"elapsed":168146,"user":{"displayName":"yam kk","userId":"06700488700849752406"}},"outputId":"35e82747-7b2a-4504-a39b-81c175161b7f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.142-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.142-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.142 ultralytics-thop-2.0.14\n"]}]},{"cell_type":"code","source":["from ultralytics import YOLO\n","\n","\n","model = YOLO(\"yolov8n-pose.pt\")"],"metadata":{"id":"dYz5Y6NPkXDd","executionInfo":{"status":"ok","timestamp":1747914207622,"user_tz":-540,"elapsed":757,"user":{"displayName":"yam kk","userId":"06700488700849752406"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import cv2\n","from google.colab.patches import cv2_imshow\n","\n","\n","capture = cv2.VideoCapture(\"/content/drive/MyDrive/cbnu/datasets/12/woman.mp4\")\n","while cv2.waitKey(10) < 0:\n","    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):\n","        capture.set(cv2.CAP_PROP_POS_FRAMES, 0)\n","\n","    ret, frame = capture.read()\n","    #cv2_imshow(frame)\n","\n","capture.release()\n","cv2.destroyAllWindows()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1DRC4DT9f15od0bsV6Lt3KsZwixEO2bUb"},"id":"7qLDkfGAlEC1","executionInfo":{"status":"error","timestamp":1747913758141,"user_tz":-540,"elapsed":8000,"user":{"displayName":"yam kk","userId":"06700488700849752406"}},"outputId":"f2f5d5e6-13ea-4ce5-be46-449f0f13651b"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","\n","\n","def predict(frame, iou=0.7, conf=0.25):\n","    results = model(\n","        source=frame,\n","        device=\"0\" if torch.cuda.is_available() else \"cpu\",\n","        iou=0.7,\n","        conf=0.25,\n","        verbose=False,\n","    )\n","    result = results[0]\n","    return result"],"metadata":{"id":"4qpZethQlFJt","executionInfo":{"status":"ok","timestamp":1747914186764,"user_tz":-540,"elapsed":6,"user":{"displayName":"yam kk","userId":"06700488700849752406"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def draw_boxes(result, frame):\n","    for boxes in result.boxes:\n","        x1, y1, x2, y2, score, classes = boxes.data.squeeze().cpu().numpy()\n","        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 1)\n","    return frame"],"metadata":{"id":"W447Du3elGPE","executionInfo":{"status":"ok","timestamp":1747914187359,"user_tz":-540,"elapsed":15,"user":{"displayName":"yam kk","userId":"06700488700849752406"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import cv2\n","from google.colab.patches import cv2_imshow\n","\n","\n","capture = cv2.VideoCapture(\"/content/drive/MyDrive/cbnu/datasets/12/woman.mp4\")\n","while cv2.waitKey(10) < 0:\n","    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):\n","        capture.set(cv2.CAP_PROP_POS_FRAMES, 0)\n","\n","    ret, frame = capture.read()\n","    result = predict(frame)\n","    frame = draw_boxes(result, frame)\n","\n","    #cv2_imshow(frame)\n","\n","capture.release()\n","cv2.destroyAllWindows()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1QKTI_zAAn875AZeHcZNjBGesB3-7PjNz"},"id":"H7MTLKxJlICV","executionInfo":{"status":"error","timestamp":1747913814048,"user_tz":-540,"elapsed":28364,"user":{"displayName":"yam kk","userId":"06700488700849752406"}},"outputId":"95a99a66-960d-4317-ffdb-bcb5921131c5"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["from ultralytics.utils.plotting import Annotator\n","\n","\n","def draw_keypoints(result, frame):\n","    annotator = Annotator(frame, line_width=1)\n","    for kps in result.keypoints:\n","        kps = kps.data.squeeze()\n","        annotator.kpts(kps)\n","\n","        nkps = kps.cpu().numpy()\n","        # nkps[:,2] = 1\n","        # annotator.kpts(nkps)\n","        for idx, (x, y, score) in enumerate(nkps):\n","            if score > 0.5:\n","                cv2.circle(frame, (int(x), int(y)), 3, (0, 0, 255), cv2.FILLED)\n","                cv2.putText(frame, str(idx), (int(x), int(y)), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1)\n","\n","    return frame"],"metadata":{"id":"gH1RvwvFxrXD","executionInfo":{"status":"ok","timestamp":1747914199685,"user_tz":-540,"elapsed":42,"user":{"displayName":"yam kk","userId":"06700488700849752406"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import cv2\n","from google.colab.patches import cv2_imshow\n","\n","capture = cv2.VideoCapture(\"/content/drive/MyDrive/cbnu/datasets/12/woman.mp4\")\n","while cv2.waitKey(10) < 0:\n","    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):\n","        capture.set(cv2.CAP_PROP_POS_FRAMES, 0)\n","\n","    ret, frame = capture.read()\n","    result = predict(frame)\n","    frame = draw_keypoints(result, frame)\n","    cv2_imshow(frame)\n","\n","capture.release()\n","cv2.destroyAllWindows()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1SvAbs-98nMHIUyOGL-WPjJEi0e0iGfu4"},"id":"qd_RQXh9yycD","executionInfo":{"status":"error","timestamp":1747914221317,"user_tz":-540,"elapsed":7811,"user":{"displayName":"yam kk","userId":"06700488700849752406"}},"outputId":"a68e67d4-b4d5-42d8-a769-f178a8808157"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Mask R-CNN\n","- COCO 데이터셋 경로 수정해서 실습 해야 함"],"metadata":{"id":"m4fgCMkFzupj"}},{"cell_type":"code","source":["import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","from pycocotools.coco import COCO\n","from torch.utils.data import Dataset\n","from pycocotools import mask as maskUtils\n","\n","\n","class COCODataset(Dataset):\n","    def __init__(self, root, train, transform=None):\n","        super().__init__()\n","        directory = \"train\" if train else \"val\"\n","        annotations = os.path.join(\"/content/drive/MyDrive/cbnu/datasets/12/coco\", \"annotations\", f\"{directory}_annotations.json\")\n","\n","        self.coco = COCO(annotations)\n","        self.iamge_path = os.path.join(root, directory)\n","        self.transform = transform\n","\n","        self.categories = self._get_categories()\n","        self.data = self._load_data()\n","\n","    def _get_categories(self):\n","        categories = {0: \"background\"}\n","        for category in self.coco.cats.values():\n","            categories[category[\"id\"]] = category[\"name\"]\n","        return categories\n","\n","    def _load_data(self):\n","        data = []\n","        for _id in self.coco.imgs:\n","            file_name = self.coco.loadImgs(_id)[0][\"file_name\"]\n","            image_path = os.path.join(self.iamge_path, file_name)\n","            image = Image.open(image_path).convert(\"RGB\")\n","            width, height = image.size\n","\n","            boxes = []\n","            labels = []\n","            masks = []\n","            anns = self.coco.loadAnns(self.coco.getAnnIds(_id))\n","            for ann in anns:\n","                x, y, w, h = ann[\"bbox\"]\n","                segmentations = ann[\"segmentation\"]\n","                try:\n","                    mask = self._polygon_to_mask(segmentations, width, height)\n","                except Exception as e:\n","                    pass\n","\n","                boxes.append([x, y, x + w, y + h])\n","                labels.append(ann[\"category_id\"])\n","                masks.append(mask)\n","\n","            target = {\n","            \"image_id\": torch.LongTensor([_id]),\n","                \"boxes\": torch.FloatTensor(boxes),\n","                \"labels\": torch.LongTensor(labels),\n","                \"masks\": torch.FloatTensor(masks)\n","            }\n","            data.append([image, target])\n","        return data\n","\n","    def _polygon_to_mask(self, segmentations, width, height):\n","        binary_mask = []\n","        for seg in segmentations:\n","            rles = maskUtils.frPyObjects([seg], height, width)\n","            binary_mask.append(maskUtils.decode(rles))\n","\n","        combined_mask = np.sum(binary_mask, axis=0).squeeze()\n","        return combined_mask\n","\n","    def __getitem__(self, index):\n","        image, target = self.data[index]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"1mrPtl-Py_vK","executionInfo":{"status":"ok","timestamp":1747915192506,"user_tz":-540,"elapsed":8,"user":{"displayName":"yam kk","userId":"06700488700849752406"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","\n","def collator(batch):\n","    return tuple(zip(*batch))\n","\n","transform = transforms.Compose(\n","    [\n","        transforms.PILToTensor(),\n","        transforms.ConvertImageDtype(dtype=torch.float)\n","    ]\n",")\n","\n","# COCO 데이터셋 경로 수정\n","dataset_root = \"/content/drive/MyDrive/cbnu/datasets/12/coco\"\n","\n","# train과 test 데이터셋 경로를 dataset_root로 수정하여 인스턴스화\n","train_dataset = COCODataset(dataset_root, train=True, transform=transform)\n","test_dataset = COCODataset(dataset_root, train=False, transform=transform)\n","\n","train_dataloader = DataLoader(\n","    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",")\n","test_dataloader = DataLoader(\n","    test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"id":"iV6e8tgCz0xD","executionInfo":{"status":"error","timestamp":1747915195759,"user_tz":-540,"elapsed":59,"user":{"displayName":"yam kk","userId":"06700488700849752406"}},"outputId":"cb854d72-729c-485e-f1c2-83ac871f3cd4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/cbnu/datasets/12/coco/annotations/train_annotations.json'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a6dcb6c97851>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# train과 test 데이터셋 경로를 dataset_root로 수정하여 인스턴스화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCODataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCODataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-c83608473c67>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/cbnu/datasets/12/coco\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{directory}_annotations.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miamge_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/cbnu/datasets/12/coco/annotations/train_annotations.json'"]}]},{"cell_type":"code","source":["from torchvision.models.detection import maskrcnn_resnet50_fpn\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","num_classes = 3\n","hidden_layer = 256\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","model.roi_heads.box_predictor = FastRCNNPredictor(\n","    in_channels=model.roi_heads.box_predictor.cls_score.in_features,\n","    num_classes=num_classes\n",")\n","model.roi_heads.mask_predictor = MaskRCNNPredictor(\n","    in_channels=model.roi_heads.mask_predictor.conv5_mask.in_channels,\n","    dim_reduced=hidden_layer,\n","    num_classes=num_classes\n",")\n","model.to(device)"],"metadata":{"id":"vWKAxozgz1jT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import optim\n","\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"],"metadata":{"id":"hPkNb_kEz0KD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1):\n","    cost = 0.0\n","    for idx, (images, targets) in enumerate(train_dataloader):\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        cost += losses\n","\n","    lr_scheduler.step()\n","    cost = cost / len(train_dataloader)\n","    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"],"metadata":{"id":"oZpDFP1hz4TD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from matplotlib import pyplot as plt\n","from torchvision.transforms.functional import to_pil_image\n","\n","\n","def draw_bbox(ax, box, text, color, mask):\n","    ax.add_patch(\n","        plt.Rectangle(\n","            xy=(box[0], box[1]),\n","            width=box[2] - box[0],\n","            height=box[3] - box[1],\n","            fill=False,\n","            edgecolor=color,\n","            linewidth=2,\n","        )\n","    )\n","    ax.annotate(\n","        text=text,\n","        xy=(box[0] - 5, box[1] - 5),\n","        color=color,\n","        weight=\"bold\",\n","        fontsize=13,\n","    )\n","\n","    mask = np.ma.masked_where(mask == 0, mask)\n","    mask_color = {\"blue\": \"Blues\", \"red\" : \"Reds\"}\n","\n","    cmap = plt.cm.get_cmap(mask_color.get(color, \"Greens\"))\n","    norm = plt.Normalize(vmin=0, vmax=1)\n","    rgba = cmap(norm(mask))\n","    ax.imshow(rgba, interpolation=\"nearest\", alpha=0.3)\n","\n","threshold = 0.5\n","categories = test_dataset.categories\n","\n","with torch.no_grad():\n","    model.eval()\n","    for images, targets in test_dataloader:\n","        images = [image.to(device) for image in images]\n","        outputs = model(images)\n","\n","        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n","        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n","        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n","\n","        boxes = boxes[scores >= threshold].astype(np.int32)\n","        labels = labels[scores >= threshold]\n","        scores = scores[scores >= threshold]\n","\n","        fig = plt.figure(figsize=(8, 8))\n","        ax = fig.add_subplot(1, 1, 1)\n","        plt.imshow(to_pil_image(images[0]))\n","\n","        masks = outputs[0][\"masks\"].squeeze(1).to(\"cpu\").numpy()\n","        masks[masks >= threshold] = 1.0\n","        masks[masks < threshold] = 0.0\n","\n","        for box, mask, label, score in zip(boxes, masks, labels, scores):\n","            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\", mask)\n","\n","        tboxes = targets[0][\"boxes\"].numpy()\n","        tmask = targets[0][\"masks\"].numpy()\n","        tlabels = targets[0][\"labels\"].numpy()\n","\n","        for box, mask, label in zip(tboxes, tmask, tlabels):\n","            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\", mask)\n","\n","        plt.show()"],"metadata":{"id":"GGxXbWzgz5X7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from pycocotools.cocoeval import COCOeval\n","\n","\n","with torch.no_grad():\n","    model.eval()\n","    coco_detections = []\n","    for images, targets in test_dataloader:\n","        images = [img.to(device) for img in images]\n","        outputs = model(images)\n","\n","        for i in range(len(targets)):\n","            image_id = targets[i][\"image_id\"].data.cpu().numpy().tolist()[0]\n","            boxes = outputs[i][\"boxes\"].data.cpu().numpy()\n","            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n","            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n","            scores = outputs[i][\"scores\"].data.cpu().numpy()\n","            labels = outputs[i][\"labels\"].data.cpu().numpy()\n","            masks = outputs[i][\"masks\"].squeeze(1).data.cpu().numpy()\n","\n","            for instance_id in range(len(boxes)):\n","                segmentation_mask = masks[instance_id]\n","                binary_mask = segmentation_mask > 0.5\n","                binary_mask = binary_mask.astype(np.uint8)\n","                binary_mask_encoded = maskUtils.encode(\n","                    np.asfortranarray(binary_mask)\n","                )\n","\n","                prediction = {\n","                    \"image_id\": int(image_id),\n","                    \"category_id\": int(labels[instance_id]),\n","                    \"bbox\": [round(coord, 2) for coord in boxes[instance_id]],\n","                    \"score\": float(scores[instance_id]),\n","                    \"segmentation\": binary_mask_encoded\n","                }\n","                coco_detections.append(prediction)\n","\n","    coco_gt = test_dataloader.dataset.coco\n","    coco_dt = coco_gt.loadRes(coco_detections)\n","    coco_evaluator = COCOeval(coco_gt, coco_dt, iouType=\"segm\")\n","    coco_evaluator.evaluate()\n","    coco_evaluator.accumulate()\n","    coco_evaluator.summarize()"],"metadata":{"id":"yyRtD1bR0FcL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bDXcg65I0GvT"},"execution_count":null,"outputs":[]}]}